\documentclass[a4paper,oneside]{report}

\usepackage[utf8]{inputenc}
\usepackage[francais]{babel}
\usepackage[top=1cm, bottom=1.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{titlesec}
%\usepackage{minted}
\usepackage[hidelinks]{hyperref}
\usepackage[backend=biber]{biblatex}
\usepackage{glossaries}
\usepackage{alltt}
\usepackage{soul}
\usepackage{color}
\usepackage{fancyvrb}

\addbibresource{rapport.bib}
\makeglossaries
\include{glossaire}

\title{Rapport \\ \og Cloud privé avec OpenStack \fg}
\author{Julien Brun, Maxime Mouchet \\ Fréderic Pourraz (tuteur)}
\date{RT2 2012-13}

\titleformat{\chapter}[hang]{\bf\huge}{\thechapter}{2pc}{}

\begin{document}

\begin{figure}
\includegraphics[width=5cm]{images/iut.png}\hfill
\includegraphics[width=5.5cm]{images/rt.png}
\end{figure}

\maketitle

% La page vide après le titre
\clearpage
\thispagestyle{empty}
\null\newpage

\chapter*{Remerciements}
\thispagestyle{empty}
\noindent Nous remercions M. Frédéric Pourraz pour nous avoir permis de réaliser ce projet.\newline
\noindent Nous remercions M. Gaëtan Ferez pour nous avoir mis à disposition le matériel et son aide sur celui-ci.

\noindent De manière générale nous remercions tout les professeurs pour leurs enseignements durant ces deux années de DUT.

\renewcommand{\contentsname}{Sommaire}
\setcounter{tocdepth}{1}
\tableofcontents

\chapter{Introduction}
\section{Choix du sujet}
La \gls{virtualisation} et le \gls{cloudcomputing} sont des solutions d'avenir pour les entreprises grâce à des coûts réduits, une maintenance simplifiée, et une montée en charge aisée.
Nous trouvons ces technologies passionnantes et mettre en place un \gls{cloudprive} nous permet de les aborder tout en approfondissant les thématiques étudiées durant notre formation.

\section{Contexte}
\subsection{La \gls{virtualisation}}
Le principe de base de la \gls{virtualisation} est de permettre le fonctionnement de plusieurs \glspl{se} (\gls{windows}, \gls{linux}, …) en simultané sur un ordinateur.
Les intérêts sont multiples, on peut citer principalement :\newline

Augmenter la disponibilité grâce à une redondance et réduire les coûts.
Par exemple on peut imaginer avoir deux serveurs mails sur une même machine physique et utiliser l’un ou l’autre en fonction de leur charge ou en cas de panne.
On réduit par la même occasion les coûts puisqu’on n’utilisera qu’une seule machine physique pour ces deux serveurs.

Un dimensionnement plus facile. La \gls{virtualisation} ajoutant une couche d’abstraction entre le \gls{se} (logiciel) et le matériel (ordinateur) on peut aisément augmenter la puissance de calcul en ajoutant, par exemple, de la mémoire sur un ordinateur sans impacter le fonctionnement des systèmes virtuels.

Faciliter les migrations site-à-site.
Il est beaucoup plus facile de transférer un serveur d’un site physique à un autre en transférant une image virtuelle par le réseau plutôt qu’en transportant un serveur physique.\newline

Il existe plusieurs logiciels permettant de virtualiser des systèmes, parmi les plus connus on pourra citer \gls{parallelsd} ou \gls{vbox}.

\subsection{Le \gls{cloudcomputing}}
\subsubsection{Vue d'ensemble}
Ces logiciels conviennent à la \gls{virtualisation} de quelques machines mais pas à la gestion global d'une infrastructure, ils se contentent de faire fonctionner plusieurs \glspl{se} sur un seul ordinateur.
Ils ne permettent pas d'utiliser des ressources et du stockage présents sur plusieurs ordinateurs depuis un lieu distant et par plusieurs personnes.

On va alors regrouper toutes ces ressources (stockage, ordinateurs, logiciels de \gls{virtualisation}) via un réseau.
On parle de \gls{cloudcomputing} et plus précisément\footnote{Voir \og IaaS, PaaS, et autres *aaS \fg ci-dessous.} dans ce cas là d’\gls{iaas}.
L'utilisateur final ne se préoccupe pas de savoir où est physiquement le stockage ou les ordinateurs qui possèdent le logiciel de \gls{virtualisation}, ni comment ils sont reliés entre eux. Il indique juste au cloud qu’il souhaite tant de machines avec tel système d'exploitation.

\subsubsection{IaaS, PaaS, et autres *aaS}
\glsreset{iaas} \glsreset{paas} \glsreset{saas}
Dans le domaine du \gls{cloudcomputing} il existe plusieurs niveaux d’abstractions, offrant des fonctionnalités plus ou moins haut niveau, on parle de modèles de services.
Leur nom prend la forme Sth- as a Service.

Les trois modèles les plus courants sont l'\gls{iaas}, le \gls{paas}, et le \gls{saas}.
Ce dernier représente le plus haut niveau d'abstraction.\newline
Dans ce modèle les clients paient un abonnement pour utiliser un logiciel dont ils n'auront pas à gérer l'installation et la maintenance.
Les applications courantes sont les \gls{crm}, la visioconférence, et les emails.
On peut citer pour exemple \gls{gapps} ou \gls{sforcecloud}.

Vient ensuite le \gls{paas}, où les applications fonctionnent également sur une plateforme externalisée (dans le Cloud) mais, à la différence du \gls{saas}, elles sont développés par le client.\newline
Cela permet aux entreprises de disposer d'un environnement d'exécution pour leur applications internes rapidement.
Par exemple, \gls{heroku}, le \gls{paas} de \gls{sforce} est capable d'exécuter de nombreux langages, tels que \gls{php}, \gls{ruby}, \gls{nodejs}, \gls{python}, \gls{java}, ou \gls{scala}.
On peut aussi citer \gls{gappengine} ou \gls{msazure}, qui offrent des fonctionnalités similaires.

Enfin vient l'\gls{iaas}, qui est le plus bas niveau d'abstraction. 
Ici le client dispose d'une infrastructure, mis à disposition par le fournisseur de Cloud, où il peut créer des serveurs virtuels à la demande, sans se préoccuper du matériel physique.
Il gère donc la configuration des systèmes d'exploitations et des logiciels qui s'y exécutent.

C'est ce dernier modèle que nous avons choisi d'étudier, nous allons donc détailler, dans la section suivante (\ref{sec:panorama_iaas}), les différentes solutions existantes avant de voir celle que nous avons retenu.

\subsubsection{Le stockage dans le cloud}
Dans le cas des \gls{iaas} on distingue deux types de stockages dans le cloud : le \gls{stockagebloc}, et le \gls{stockageobjet}.
Le premier est proche du matériel physique, limitant les couches d'abstractions, et offre des performances de haut-niveau.
Il est exposé aux instances virtuelles comme un disque physique natif, ce qui permet au système d'exploitation de démarrer dessus.\newline
Le \gls{stockageobjet}, au contraire, est dit \og haut-niveau \fg car il est accessible via une \gls{api} et l'utilisateur final ne se préoccupe pas de savoir où, ni comment, ses données seront stockés.
Il est généralement plus lent que le \gls{stockagebloc} mais disponible en plus grande quantité et redondé, il est donc utilisé pour les données qui changent peu (images de machines virtuelles, sauvegardes, photos, ...).

\subsubsection{Cloud public vs. \gls{cloudprive}}
La différence majeure entre un Cloud dit \og public \fg, d'un Cloud dit \og privé \fg est le partage de l'infrastructure.
Dans le premier cas plusieurs clients se partagent l'infrastructure, tandis que dans le second seul une entreprise utilise l'infrastructure.\newline
Un Cloud public sera toujours situé en dehors de l'entreprise, alors qu'un \gls{cloudprive} pourra être installé dans l'entreprise et géré par elle-même.
Il en résulte alors un contrôle complet de l'infrastructure et une sécurité accrue.

\subsection{Panorama des IaaS} \label{sec:panorama_iaas}
\subsubsection{\gls{amazon} EC2}
\gls{amazon} a lancé \gls{ec2} durant l'été 2006.
Il s'agit d'un \gls{cloudpublic}, au sens où l'infrastructure est commune à tout les clients, et accessible à travers une \gls{api} et une interface Web, moyennant un paiement par heures d'utilisation.

Les instances, allant de 1 à 244Go de RAM et de 1 \gls{coeur} virtuel à 32 \glspl{coeur} physiques, sont virtualisés grâce à l'hyperviseur Open Source \gls{xen}.
\gls{ec2} supporte \gls{linux}, \gls{bsd}, \gls{solaris}, et \gls{windows} comme \glspl{se} invités.

Le \gls{stockagebloc} est fourni par \gls{amazon} \gls{ebs} tandis que le \gls{stockageobjet} est fourni par \gls{amazon} \gls{s3}.
Chacun de ces service possède sa propre \gls{api}, permettant de les utiliser indépendamment les uns des autres.

A titre informatif, \gls{ec2} est utilisé, entre autres, par \gls{netflix}\footnote{En 2011, aux heures de pointes, \gls{netflix} était responsable de 29\% du trafic Internet en Amérique du Nord \cite{NetflixTrafic}. D'où la nécessité d'une infrastructure robuste et scalable.}, \gls{instagram}, et \gls{shazam}, tandis que la plupart des services de stockage en ligne grand public, comme \gls{dropbox}, utilisent \gls{s3}.

Il faut noter que l'\gls{iaas} d'\gls{amazon} est propriétaire et spécifique à leur infrastructure. Il n'est pas possible de la déployer sur son propre matériel.

\subsubsection{OpenNebula}

\subsubsection{CloudStack}
\gls{cloudstack} est un système de création d'\gls{iaas} originellement développé par \gls{cloudcom} puis racheté par \gls{citrix} à l'été 2011.
Il appartient désormais à la \gls{fondationapache} et est donc \gls{opensource} sous la licence correspondante\footnote{La \gls{licenceapache}}.\newline
CloudStack est développé en \gls{java}.

\subsubsection{OpenStack}
\gls{openstack} a été lancé en Juillet 2010 par Rackspace et la NASA.
Son but est de proposer un outil gratuit et libre de création d'\gls{iaas}.
Le code source du projet (du \gls{python}) est disponible sur GitHub\footnote{https://github.com/openstack} et Launchpad\footnote{https://launchpad.net/openstack} sous licence Apache 2.0.\newline
Outre ses deux fondateurs OpenStack est à l'heure actuelle principalement utilisé par HP, AT\&T, Intel, et Canonical.

\section{Rappel du cahier des charges}
\subsubsection{Objectifs}
Dans un premier temps nous voulions mettre en place un \gls{cloudprive} sous la forme d'une \gls{iaas} avec \gls{openstack}.
L'objectif était de permettre à des utilisateurs (des élèves par exemple) de créer des machines virtuelles très rapidement même sur des machines disposant de peu de ressources.\\

Ensuite nous avons voulu mettre en place une solution pour automatiser la configuration des instances.
L'objectif était de permettre à un/des administrateur(s) de modifier une configuration ou de rajouter une application même après qu'une image ait été créé.

\subsubsection{Fonctionnalités à implémenter}
\begin{itemize}
\item Un noeud de \gls{virtualisation} avec \gls{kvm}
\item Authentification \& Autorisations avec un serveur \gls{ldap}
\item Interface simple d'utilisation pour la création d'images
\item Personnalisation des instances au lancement suivant l'utilisateur (montage des disques, ...)
\end{itemize}

\subsubsection{Perspectives}
\begin{itemize}
\item Haute-disponibilité, tolérance de pannes
\item Monitoring des machines physiques
\item Nœuds de \gls{virtualisation} supplémentaires avec \gls{xen}, \gls{lxc}, voir \gls{hyperv}
\item Migration automatique des VMs après la chute d'un noeud
\end{itemize}

\section{Démarche}
Initialement notre projet devait comporter deux grandes étapes: la mise en place d'une installation de développement, et le déploiement sur un \gls{bladecenter}. Après cette dernière étape et discussion avec notre tuteur, nous avons décidé de modifier l'installation que nous avions réalisé sur le \gls{bladecenter} afin de maximiser les possibilités de \gls{virtualisation} et de simplifier l'administration.\newline
Au final ce sont donc ces trois grandes étapes que nous détaillons dans le chapitre \ref{cha:miseenplace}.

De plus la majorité de notre travail a été effectué à deux.
Notre sujet étant très pointu et compliqué. 
Nous avons donc préféré être à deux sur la majorité des tâches afin de pouvoir mettre en commun nos idées et réfléchir ensemble à la résolutions des problèmes rencontrés pendant des différentes installations.


Dans la suite de ce rapport nous commencerons par détailler le matériel et les logiciels utilisés, puis nous verrons comment nous les avons assemblés afin d'obtenir notre \gls{cloudprive}.


\chapter{Matériel}

\section{Serveur dédié OVH}
Nous avons réalisé la première partie de notre projet sur un serveur dédié Kimsufi d'OVH.
Il était équipé d'un processeur 64 bits à 3,4Ghz avec les extensions VT-x (Intel Core i3), de 8Go de RAM, et de 2x1To de stockage en \gls{raid0}.

Il s'agit de la configuration minimale pour faire fonctionner une installation de développement d'\gls{openstack}.

\section{BladeCenter HP}
Une fois notre projet fonctionnel sur le serveur dédié nous avons déployé une nouvelle installation d'\gls{openstack} sur quatre lames du \gls{bladecenter} dont dispose notre département au sein de l'IUT.

La configuration est cette fois-ci beaucoup plus intéressante et permet d'entrevoir les réelles possibilités du \gls{cloudcomputing}\footnote{Toutefois, bien que le \gls{bladecenter} possède une puissance importante, ses composants sont assez anciens et ne supportent pas les technologies de \gls{virtualisation} matérielle (VT-x, VT-d, AMD-V), ce qui limite le nombre de systèmes invités possibles.}.

\subsection{Configuration matérielle}
Nous disposions de trois lames équipés chacune de deux processeurs AMD Opteron 250 (2,4GHz / 64 bits) et de 4Go de RAM, ainsi que d'une lame équipé de deux Opteron 275 (2,2GHz / 64 bits) et de 8Go de RAM.\newline
Toutes les lames possèdent deux cartes \gls{fc} QLogic Q2312 pour le stockage.

\subsection{Stockage}
Les lames ne disposant pas de stockage intégré, ce dernier est assuré pas un \gls{san} HP StorageWorks relié en \gls{fc}.
Ce dernier fournit une couche d'abstraction au-dessus des disques durs et permet d'exposer plusieurs disques \og virtuels \fg à une lame au travers du contrôleur \gls{fc}.

PARLER DU MULTIPATH----------------

\subsection{Réseau}
Chaque lame possède deux liens à 1Gbit/s que nous avons agrégés afin de faire du \gls{load balancing} et du \gls{failover}. Cela présente deux avantages: dans le meilleur cas on obtient une bande passante totale de 2Gbit/s, dans le pire cas (chute d'un lien) le trafic continue de fonctionner (à 1Gbit/s).

La séparation des réseaux est permise par l'ajout de tag sur des trames Ethernet avec un numéro de \gls{vlan} (encapsulation 802.1Q).


\chapter{Systèmes d'exploitations hôtes}
\section{Ubuntu Server} \label{cha:ubuntu}

\section{CentOS}

\section{Solaris}

\subsection{Les zones}


\chapter{Logiciels}
\section{OpenStack}
Nous avons choisi OpenStack car des solutions d'IaaS présentés précédemment dans la section \ref{sec:panorama_iaas} c'est celle qui est soutenu par le plus d'entreprise, et, comme en témoigne le volume des recherches Google, elle possède une communauté très développé sur Internet.
\begin{figure}[h!]
\centering
\includegraphics[resolution=140]{images/recherches_iaas.png}
\caption{Evolution du volume des recherches Google pour les principales IaaS}
\end{figure}


\subsection{La fondation OpenStack}

\subsection{Les différentes versions d'OpenStack}


\subsection{Les services}
\gls{openstack} est constitué de différents services gérant chacun une composante spécifique de l'\gls{iaas}.
\subsubsection{Keystone}
Ce service est indispensable, il permet de coordonnée l'authentification et l'accès aux autres services.\newline
Il expose une \gls{api} HTTP(S) sur deux endpoints, un public sur le port 5000, permettant l'authentification des utilisateurs, et un autre dit privé sur le port 35357 permettant les tâches administratives tels que la gestion des utilisateurs et des terminaisons de services.
Il supporte différents backends pour l'authentification, certains permettant la persistance et l'édition des données comme \gls{sql} ou \gls{ldap}, d'autres en lecture-seule comme \gls{pam}.\newline

\subsubsection{Nova}
C'est le service qui va gérer les systèmes de \gls{virtualisation} (\gls{kvm}, \gls{xen}, ...).
Il est chargé de créer/démarrer/arrêter les machines, et de gérer les ressources physiques qui leurs sont allouées, ainsi que le stockage bloc si Cinder n'est pas utilisé, et le réseau si Quantum n'est pas utilisé.\newline
Il utilise Keystone pour l'authentification.


\subsubsection{Quantum}


\subsubsection{Cinder}
Depuis la version Folsom le \gls{stockagebloc} possède son propre service indépendant 



\subsubsection{Swift}
Swift est le système de \gls{stockageobjet}. 

Il supporte deux systèmes d'authentification, Swauth hérité de Rackspace, et Keystone.

\subsubsection{Glance}
Glance s'occupe des images virtuelles\footnote{Une image est le modèle d'après lequel serons créer les machines.}.
Il enregistre leur caractéristiques et leur emplacements de façon à ce que les autres services n'aient pas à s'en occuper.

Il utilise Keystone pour l'authentification et peut, en plus du système de fichier local, utiliser Swift pour stocker les images.

\subsubsection{Dashboard}
C'est l'interface graphique qui permet à l'utilisateur final de gérer ses instances virtuelles.

\section{Automatisation}

Une fois la ou les machines virtualisées, il faut les configurer et installer des applications. On peut effectuer ces tâches à la main mais il existe des solutions libre qui permettent d'automatiser ces tâches.

\subsubsection{Juju}
Canonical, la société qui édite Ubuntu (voir \ref{cha:ubuntu}) distribue, sous licence libre, Juju, un service d'orchestration
\footnote{Un service d'orchestration est un programme qui permet d'automatiser la coordination et l'organisation de systèmes complexe.}, 
écrit en python, qui permet de configurer automatiquement des services
\footnote{Dans juju, un service est appelé un "charme". Un charme peut aussi bien être un programme comme Apache ou Varnish ou un service web comme MediaWiki} 
et de créer des relations entre eux. 
Juju rajoute une couche d'abstraction entre la machine et le service. Il suffit de définir des relations entre les charmes et ils se configurent automatiquement. 
Par exemple, il suffit d'établir une relation entre un logiciel de load balancing (comme Varnish) et notre service (MediaWiki par example) pour que la charge soit répartie sur les différents serveur, quelque soit le nombre d'instance du service (ici MediaWiki).
Juju s'installe sur l'ordinateur de l'utilisateur et s'utilise en ligne de commande mais il existe une interface web (en beta à l'heure actuel) qui s'installe sur un serveur distant.
Juju supporte la plus part des Iaas public du marché (Hp, Amazon et Rackspace) et aussi les Iaas privé qui utilise OpenStack.
L'annexe \ref{auto:juju} explique comment déployer plusieurs instances de MediaWiki avec quelque dépendance (MySql et HAProxy)
\newline\\
Juju est très pratique pour déployer rapidement un environnement complet et durable dans le temps. 
Cependant, il l'est beaucoup moins pour appliquer une configuration avancé sur une multitude de machine.
Pour ce genre de besoin, il existe des solutions comme Puppet, Chef ou encore CfEngine qui permettent d'automatiser ces configurations et de les appliquer simultanément sur plusieurs machines.

\subsubsection{Puppet} 
Nous avons choisi d'utiliser Puppet car il possède une communauté très active et la syntaxe de ses scripts de configuration nous semble la plus clair.
Puppet marche sur un système de clients serveur.
Chaque machine à configurer, appelé nœud, a le client puppet installé.
Elles doivent s'identifier auprès du serveur, appelé PuppetMaster.
Les communication entre le PuppetMaster et ses clients est chiffré grâce à des certifiera généré par puppet à son démarrage.
Afin de définir les configurations à effectuer sur chaque nœud, on utilise des recettes.(Voir l'annexe \ref{auto:puppet} pour une explication concrète d'utilisation d'une recette)
Nous avons choisi de les stocker sur un serveur git afin de mieux gérer les différentes révisions et l'édition distante.

\section{Haute disponibilité}
\subsection{GlusterFS}

\subsection{Heartbeat}

\subsection{Pacemaker}

\section{Reverse Proxy}
Pour les première installation, nous avons décidé de ne connecter directement aucun service avec l'extérieur pour des raisons de sécurité, de contrainte technique (une seul ip disponible sur le réseau de l'IUT) et de facilité de mise en œuvre (configuration du pare-feu simplifié).
De cette manière, la zone qui abrite le dashboard n'est connecté qu'au LAN interne des lames.
De cette manière, il nous faut un moyen de rendre le dashboard accessible depuis l'extérieur. 
Nous avons choisi de mettre une place une mécanique de reverse proxy\footnote{Nous aurions pu décider de translater le port 80 (HTTP) sur l'ip de la zone abritant le serveur web, mais cette solution aurait été moins souple.
En effet, la translation ne nous autorise qu'une seul redirection alors que le reverse proxy se base sur les données HTTP et non le port. De cette manière, il nous permet de crée autant de sous domaine que l'on veux et de les rediriger vers n'importe quel ip et port.}.

Un reverse proxy est un serveur qui permet de réécrire les requêtes HTTP afin, par example, de cacher à l'utilisateur l'architecture interne du réseau.
Grâce à lui, nous pouvons rediriger les requêtes vers notre serveur web.
Un reverse proxy permet aussi de mettre en cache les ressources statique d'un site pour l'accélérer et soulager les serveurs web ou encore répartir la charge entre plusieurs serveurs.

Il existe plusieurs programme qui permette de faire du reverse proxy.
Le mieux, de par sa communauté, la finesse de ces règles, et ces performances est, à notre avis, Varnish\footnote{https://www.varnish-cache.org/}.
C'est un logiciel libre utilisé par example par Facebook. Cependant, nous n'avons pas pu l'utiliser pour les raisons détaillé dans la partie dédié.

Nous avons donc décidé d'utiliser Apache comme reverse proxy. Apache est le serveur web le plus utilisé\footnote{D'après le site netcraft.com, en Février 2013, Apache est utilisé par près de 55\% des sites actif. (IIS de Microsoft et Nginx sont eux à 12\%)} dans le monde, à l'heure actuel.
Il propose une fonction de reverse proxy grâce au module mod\_proxy. Sa configuration un peu moins fine que Varnish mais suffisante pour notre utilisation.

\section{Supervision}

Ajouté Munin

Afin d'être sur qu'il n'y aucun problème à la fois sur le réseau et sur les machines, il existe des programmes appelé logiciel ou plateforme de supervision qui, en s'appuyant sur le protocole snmp, vérifie en continu le bon fonctionnement de tous les équipements réseaux et de tous les logiciels installé sur les machines.
Ils peuvent vérifier de la place restante sur les disques jusqu'à l'état des interfaces réseaux des switchs, en passant par le nombre de clients connecté à la base MySQL.
Lorsque un problème est détecté, une alerte est émise\footnote{Soit par mail, soit par sms... Tout dépend du logiciel de supervision et de sa configuration.}

Le marché de la supervision est à la fois occupé par de grand constructeur comme HP avec leur gamme OverView et par des logiciels libres comme Zabbix ou Nagios.
Nous avons choisi pour notre part d'utiliser Shinken\footnote{http://www.shinken-monitoring.org/}.
Il a l'avantage de s'installer facilement, d'être compatible avec Solaris\footnote{Shinken est écrit en python ce qui le rend disponible pour presque tous les systèmes.} et d'être compatible avec les plugins nagios et ainsi de bénéficier de son l'immense communauté.



\chapter{Mise en place} \label{cha:miseenplace}
\section{Installation de développement sur un serveur dédié}
Nous avons réalisé une première installation d'\gls{openstack} et des services associés sur un serveur dédié afin d'étudier le fonctionnement global.
Dans cette section nous ne détaillerons que les configurations des systèmes d'exploitations, et pas l'installation d'\gls{openstack} qui sera abordé dans la section suivante sur le \gls{bladecenter}.

\subsection{Environnement de test}
Afin de simuler plusieurs serveurs physiques nous avons installés \gls{proxmox} sur le serveur dédié. Il s'agit d'une distribution \gls{linux} associant \gls{kvm} et \gls{openvz} à une interface web afin de permettre la création rapide de machines virtuelles.

A l'aide de \gls{proxmox} nous avons donc créés deux machines virtuelles fonctionnant sur \gls{kvm}: \emph{node-controller} et \emph{node-compute}. Après avoir effectués une installation minimale d'Ubuntu Server 12.04 LTS sur les deux machines nous avons installés \gls{openstack} conformément aux instructions du manuel \og OpenStack Install and Deploy Manual - Ubuntu \fg \cite{OSFolsomInstallUbuntu1210}.

\subsection{Conclusion}
Cette installation nous as permis de commencer à travailler rapidement car il s'agissait d'une configuration standard et les préliminaires à l'installation d'\gls{openstack} (installation des systèmes d'exploitations, configuration du réseau) nous ont pris très peu de temps.
Elle était donc idéale à des fins d'étude et de développement.\newline
Cependant il ne s'agit pas d'une configuration utilisable en production car beaucoup trop lente du fait du peu de ressources disponibles.

\section{Déploiement sur un BladeCenter avec Solaris} \label{sec:depsolaris}

Par souci de clarté, nous avons décidé de donner un nom (hostname) au trois lames que nous utilisons afin de les différencier plus facilement.
Les noms sont donnés dans le schéma global (Annexe \ref{sch:glob}).
Par la suite, lorsque nous parlerons d'une lame en particulier, nous l'appellerons par son nom.

\subsection{Préliminaires}
Nous avons choisi d'installer Solaris 11 sur Fuji et Iyo et CentOS 6 sur Raijin.
Pour plus d'informations sur ce choix voir \ref{sec:compatblade}.

L'installation des systèmes est détaillé dans les annexes \ref{sec:installcentos} et \ref{sec:installsolaris}.

\subsubsection{Configuration du stockage}
Parler du multipath etc..

\subsubsection{Configuration du réseau}
Comme expliqué précédemment, les lames Iyo, Tenjin et Raijin sont connecté en 2Gbit/s (deux interfaces à 1Gbit/s agrégés comme expliqué dans cet article \cite{SolarisLA}) dans un VLAN dédié et non-relié à l'extérieur.
Fuji, quand à elle, a une interface sur le réseau extérieur (réseau de IUT avec accès à internet) et l'autre dans le LAN inter-lames.
De cette manière, tous le trafic transite par elle et peut être redirigé suivant les règles du reverse proxy et du pare-feu. 

Afin de donner accès à internet au lames du réseau interne, nous avons mis en place une translation d'adresse sur Fuji (NAT).
Sur Solaris, le pare-feu par défaut est ipfilter. La configuration (donné en annexe \ref{conf:NAT}) permet de translater l'adresse locale (192.168.1.0/24) vers l'adresse public (10.102.75.190).

Maintenant que nos machine on accès à internet, il faut qu'on rende disponible depuis l'extérieur les interfaces web des différent services disponible (Le dashboard, le panel Shinken et l'interface de configuration de Puppet).
Pour réaliser cela, nous allons utiliser un reverse proxy (expliqué précédemment). Une fois apache installé, il faut ajouter au fichier de configuration d'apache (httpd.conf) les directive de l'annexe \ref{conf:apacheProxy}.
Celle-ci renvoie tous le trafic qui arrive à apache vers la machine qui a l'ip 192.168.1.3 et sur sont port 80.

\subsubsection{Environnement de travail}
Afin de rendre l'utilisation des machines plus agréable et plus facile à configurer, nous avons ajouté plusieurs programme et modifié plusieurs configuration.

Nous avons commencé par installer un gestionnaire de paquet.
Solaris possède sont propre gestionnaire de paquet que l'on peut appeler par la commande pkg. Seulement, il y a assez peu de paquet disponible.
Une communauté s'est alors créé autour du projet OpenCSW, un gestionnaire de paquet qui contient un très grand nombre\footnote{le projet OpenCSW propose 3675 paquet en février 2013} de paquet compilé pour Solaris.
Les paquets s'installent très facilement avec la commande pkgutils et les dépendances sont bien géré. Nous avons pu grâce à OpenCSW, installer de nombreux paquets comme Zsh, Apache ou encore Tmux\footnote{Tmux est un multiplexeur de terminaux
Il permet d'avoir plusieurs terminaux dans la même fenêtre (et donc sur la même connexion \gls{ssh} dans notre cas)} sans avoir à les compiler avec toutes leurs dépendances. 

Pour pouvoir configurer nos différente machines, nous nous connectons à elle via le protocole \gls{ssh}. 
C'est un protocole de communication sécurisé qui permet d'avoir un terminal distant sur une machine.
Notre configuration réseau nous permet d'accéder depuis le réseau de l'IUT seulement à Fuji.
Nous nous connectons donc depuis notre ordinateur personnel sur Fuji puis de Fuji vers une autre machine du réseau local.
Il est aussi possible de faire des tunnel \gls{ssh} (voir \ref{tunnelingSsh})

\subsection{Installation des zones}

\subsection{Mise en place d'OpenStack}
\subsubsection{Préliminaires}

\subsubsection{Installation du service d'identité}

\subsubsection{Installation du service d'images}

\subsection{Conclusion}
S'agissant de notre premier contact avec un BladeCenter un certains nombre de concept étaient nouveaux pour nous (\gls{fc}, \gls{san}, multipath) et nous manquions de recul sur le dimensionnement de notre installation.
C'est pourquoi, après discussion avec notre tuteur, nous avons décidé de reprendre l'installation de manière beaucoup plus propre.

En effet bien que Solaris fonctionne parfaitement sur les lames son administration diffère beaucoup des systèmes Linux et la pérennité de l'installation n'est donc pas garantie.\newline
De plus notre configuration réseau nous obligeait à faire du NAT pour accéder aux machines virtuelles et ce n'est pas une configuration idéale, surtout dans le cas où les utilisateurs ont besoins d'accéder plusieurs ports sur une machine virtuelle.


\section{Déploiement sur un BladeCenter avec CentOS}
Comme dans la section \ref{sec:depsolaris} nous avons donné des noms (hostname) aux lames afin de les différencier plus facilement que par leur IP.
Ces noms sont détaillés dans le tableau ci-dessous ainsi que DANS LE SCHEMA ??????

Nous avons cette fois ci voulu réaliser une installation permettant de maximiser les capacités de virtualisation et de résister à la chute éventuel d'un noeud.
Pour cela nous nous sommes basés sur ce guide https://github.com/beloglazov/openstack-centos-kvm-glusterfs que nous avons adapté et auquel nous avons rajouter plusieurs fonctionnalités. LESQUELS????

PARLER DU RESEAU / CLOUD CONTROLLER / GlusterFS /...

\subsection{Préliminaires}
\subsubsection{Installation des \gls{se}}
Nous avons choisi d'installer CentOS 6.3 x64 sur les quatres lames. Le procédure d'installation est décrite dans l'annexe \ref{sec:installcentos}.\newline
Il s'agit d'une installation sans Internet car cette fois-ci les liens sont en mode trunk et nous tagguons nous-même les VLANs.

\subsubsection{Configuration des dépôts YUM}
Les dépots de paquets par défaut de CentOS ne contiennent pas OpenStack. Il faut donc ajouter le dépot EPEL\footnote{C'EST QUOI CA????,}, pour celà:
\begin{verbatim}
sudo rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm
\end{verbatim}

\subsection{Installation du Cloud Controller}
\subsubsection{Installation du serveur MySQL}
\begin{verbatim}
yum install -y mysql mysql-server

# On change le mot de passe root de mysql
mysqladmin -u root password PASS_MYSQL
\end{verbatim}
\subsubsection{Installation du service d'identité}
Il faut d'abord créer la base dans MySQL:
\begin{verbatim}
mysql -u root -p

CREATE DATABASE keystone;
GRANT ALL ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'PASS_DBKEYSTONE';
GRANT ALL ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'PASS_DBKEYSTONE';
\end{verbatim}

On peut maintenant installer Keystone:
\begin{verbatim}
yum install -y openstack-utils openstack-keystone
\end{verbatim}

Pour configurer Keystone voir l'annexe \ref{conf:Keystone}.

Fin de la configuration:
\begin{verbatim} 
# On crée les tables dans la DB
keystone-manage db_sync

# On mets les bonnes permissions (le fichier de config contient le pass SQL !)
chmod 640 /etc/keystone/keystone.conf 
chown -R keystone:keystone /var/log/keystone
chown -R keystone:keystone /var/lib/keystone

# On démarre le service et on l'active au démarrage
service openstack-keystone restart
chkconfig openstack-keystone on
\end{verbatim}

On va utiliser une version modifié de keys???????????????


\subsubsection{Installation du services d'image}
\noindent On commence par créer la base MySQL:
\begin{verbatim}
mysql -u root -p

CREATE DATABASE glance;
GRANT ALL ON glance.* TO 'glance'@'%' IDENTIFIED BY 'PASS_DBGLANCE';
GRANT ALL ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'PASS_DBGLANCE';
\end{verbatim}

\noindent On installe Glance:
\begin{verbatim}
yum install -y openstack-glance
\end{verbatim}

\noindent Pour configurer Glance voir l'annexe \ref{conf:Glance}.

\noindent Fin de la configuration:
\begin{verbatim}
# On crée les tables dans la DB
glance-manage db_sync

# On mets les bonnes permissions
chmod 640 /etc/glance/*.conf
chmod 640 /etc/glance/*.ini
chown -R glance:glance /var/log/glance
chown -R glance:glance /var/lib/glance

# On démarre les services et on les active au démarrage
service openstack-glance-registry restart
service openstack-glance-api restart
chkconfig openstack-glance-registry on
chkconfig openstack-glance-api on
\end{verbatim}*

\noindent Pour la création des utilisateurs, services, et endpoints, voir l'annexe \ref{script:keystoneData}.

\subsubsection{Installation de Nova}
\noindent On crée la base MySQL:
\begin{verbatim}
mysql -u root -p

CREATE DATABASE nova;
GRANT ALL ON nova.* TO 'nova'@'%' IDENTIFIED BY 'PASS_DBNOVA';
GRANT ALL ON nova.* TO 'nova'@'localhost' IDENTIFIED BY 'PASS_DBNOVA';
\end{verbatim}

\noindent On installe Nova:
\begin{verbatim}
yum install -y openstack-nova* qpid-cpp-server
\end{verbatim}

\noindent Pour configurer Nova voir l'annexe \ref{conf:Nova}.

\noindent Fin de la configuration:
\begin{verbatim}
# On crée les tables dans la DB
nova-manage db sync

# On mets les bonnes permissions
chmod 640 /etc/nova/nova.conf
chown -R root:nova /etc/nova
chown -R nova:nova /var/lib/nova

# On démarre les services et on les active au démarrage
service qpidd restart
service tgtd  restart
service openstack-nova-api          restart
service openstack-nova-cert         restart
service openstack-nova-consoleauth  restart
service openstack-nova-direct-api   restart
service openstack-nova-metadata-api restart
service openstack-nova-scheduler    restart
service openstack-nova-volume       restart
chkconfig qpidd on
chkconfig tgtd  on
chkconfig openstack-nova-api          on
chkconfig openstack-nova-cert         on
chkconfig openstack-nova-consoleauth  on
chkconfig openstack-nova-direct-api   on
chkconfig openstack-nova-metadata-api on
chkconfig openstack-nova-scheduler    on
chkconfig openstack-nova-volume       on
\end{verbatim}

\subsection{Installation des noeuds de Compute}



\subsection{Conclusion}


\chapter{Problèmes rencontrés}
\section{Compatibilité des systèmes d'exploitations sur le BladeCenter}
\label{sec:compatblade}
L'installation des systèmes d'exploitation sur les lames fût plus compliquée que prévu, étant donné la configuration bi-processeur et les cartes Fibre Channel.

Après de multiples installations infructueuses, un appel au support HP et la vérification des versions des firmwares nous sommes arrivés à la conclusion que les noyaux Linux récents ($ > 2.6 $) sont trop instables.

Sans que nous puissions fournir d'explications, CentOS, dans sa dernière version (6.3), fonctionne parfaitement sur la lame équipée d'Opteron 275, alors qu'aucun autre système Linux (Debian, Ubuntu, Suse) n'y fonctionne.
Il suffit de préciser linux text mpath au prompt boot de l'installateur.

Sur les autres lames nous avons installé Solaris 11 qui fonctionne parfaitement et supporte le multipath/boot on SAN nativement.

\section{Versions de librairies incompatible sur Solaris}
\label{sec:libsolaris}

\chapter{Conclusion finale}
La mise en place d'une infrastructure de \gls{cloudcomputing} demande de nombreuses connaissances dans le domaine de l'informatique et des réseaux.
Nous avons appris énormément durant la réalisation de ce projet mais nous avons clairement manquer de recul dans nos choix et avec de l'expérience nous aurions pu être plus efficaces.\newline
Cependant nous avons pu implémenter toutes les fonctionnalités du cahier des charges y compris les fonctionnalités supplémentaires tels que la haute disponibilité et le monitoring, à l'exception de l'authentification avec le LDAP car non réalisable avec le serveur de l'Université de Savoie\footnote{Keystone requiert des droits en écriture et une structure de l'arbre particulière, ce qui n'est évidemment pas possible sur le LDAP de l'Université.}.

Concernant \gls{openstack} il s'agit d'un projet au potentiel énorme de part sa flexibilité (services indépendants), sa portabilité (Python), et sa communauté (\emph{mailing lists} entre autres) mais qui est encore en développement actif et dont la compatibilité n'est pas assurée d'une version à l'autre.




\appendix
\chapter{Utilisation du \gls{cloudprive}}
\section{Généralités}
\subsection{Dashboard}
Le dashboard d'OpenStack qui offre une interface conviviale pour utiliser et administrer le cloud se trouve à l'adresse http://10.102.75.100/dashboard .

\subsection{Réseau privé vs. Réseau public}

\subsection{Keypair}

\subsection{VNC}


\section{Création d'une instance virtuelle}
\begin{figure}[!h]
\centering
\includegraphics[resolution=200]{images/dashboard_create_details.png}
\caption{Exemple de création de huit instances virtuelles via le dashboard}
\end{figure}

\section{Création d'un conteneur de stockage}

\chapter{Administration du \gls{cloudprive}}

\section{Gestion des utilisateurs}

\section{Dépannage à l'aide des logs} 


\chapter{Installation des systèmes d'exploitations sur les lames} \label{sec:installos}
Les lames du BladeCenter ne disposant pas de lecteur optique et l'accès physique n'étant pas toujours faisable, il est possible de réaliser l'installation des systèmes d'exploitation à distance.
Nous détaillerons ici les fonctionnalités intégrés aux lames permettant de les gérer à distance, ainsi que l'installation des différents systèmes que nous avons utilisés.\newline
Note: les installations détaillés sont spécifiques au BladeCenter et correspondent à la configuration minimum pour faire fonctionner OpenStack. Des documentations plus exhaustives sont disponibles sur les sites respectifs des OS.

\section{Systèmes d'exploitations supportés}
\subsubsection{Systèmes officiellement supportés}
D'après HP les systèmes d'exploitations supporté sur les lames BL35p sont:
\begin{itemize}
\item Red Hat Enterprise Linux (RHEL) 5
\item Suse Linux Enterprise Server (SLES) 10
\item VMware ESX 2.5.5
\item Solaris 10
\end{itemize}

\subsubsection{Systèmes que nous avons testés}
Après de multiples essais nous avons conclus que les systèmes suivants sont stables et supportent le multipath en natif sur les lames:
\begin{itemize}
\item Solaris 11 (64 bits)
\item CentOS 6.3 (64 bits)
\item VMware ESXi 4.1 (dernière version installable, les versions suivantes requièrent les extensions processeurs LAHF et SAHF)
\end{itemize}

\section{Integrated Lights-Out}
Chaque lame possède un serveur de gestion intégré, \gls{ilo}, permettant de gérer l'alimentation (allumage, extinction, reset), de connecter des médias virtuels, et fournissant un \gls{kvmphys} dans le navigateur.
Les \gls{ilo} sont sur un réseau privé en 10.100.100.0/24 qui est accessible depuis un serveur TSE à l'adresse manap.rt.iut-acy.local (10.102.75.190).

\subsection{Connexion}
\subsubsection{Serveur TSE}
Le serveur TSE manap.rt.iut-acy.local est accessible depuis tout poste de l'IUT à l'adresse 10.102.75.190:3389. \gls{windows} XP,Vista,7 et 8 possèdent un client RDP nommé "Connexion Bureau à distance". Sur Linux on pourra utiliser Remmina qui intègre le tunneling SSH (voir paragraphe suivant).


Depuis l'extérieur (WiFi, maison) il est nécessaire d'utiliser le VPN\footnote{Le téléchargement et l'installation du client VPN sont détaillés sur vpn.univ-savoie.fr} mais, celui-ci bloquant les ports peu utilisés (pour des raisons de sécurité), il faut également mettre en place un \gls{tunnelssh}\footnote{NANANANNA}.
Pour ce faire il faut d'abord demander un accès à srv-dev.iut-acy.local à la DSI puis configurer son client \gls{ssh}.\newline
Sur \gls{windows} on utilisera Putty\footnote{Les instructions de configuration du \gls{tunnelssh} sous \gls{windows} sont disponibles à l'adresse https://w3.iut-acy.univ-savoie.fr/institutionnel/service-informatique/faq/tunnel-ssh/} tandis que sur Linux, à défaut de client gérant le tunneling comme Remmina, on utilisera la commande ssh comme suit pour mapper le port local 10389 sur le port 3389 du serveur TSE:
\label{tunnelingSsh}
\begin{verbatim}
ssh -L 10389:localhost:10.102.75.190:3389 login@srv-dev.iut-acy.local
\end{verbatim}
Il suffira alors d'indiquer l'adresse 127.0.0.1:10389 dans le client RDP pour se connecter.

Le serveur, fonctionnant sous \gls{windowsserver} 2003, possède Firefox comme navigateur Internet, et plusieurs autres logiciels utiles, comme le client VMWare vCenter pour la gestion d'ESXi.

\subsubsection{iLO}
Il suffit d'indiquer l'adresse IP correspondant à l'iLO de la lame dans le navigateur et d'entrer ses identifiants.
\newpage

\subsection{Principales fonctionnalités}
\subsubsection{Gestion de l'alimentation}
La section \emph{Virtual Power} de l'onglet \emph{Virtual Devices} indique l'état actuel de l'alimentation de la lame et propose six options:
\begin{description}
\item[Momentary Press:] Équivalent à un appui court sur le bouton Power de la lame.
Permet de démarrer ou d'arrêter proprement le système.
\item[Press and Hold:] Équivalent à un appui de cinq secondes sur le bouton Power de la lame.
Permet d'éteindre la lame si le système ne répond plus.
\item[Cold boot of system:] Coupe l'alimentation pendant six secondes puis allume la lame.
Permet aux condensateurs de se décharger et ainsi de résoudre certains problèmes matériels.
\item[Reset system:] Redémarre la lame.
\item[Manual Override for BL p-Class:] Force le mise sous tension de la lame sans tenir compte de la puissance électrique disponible.
Peut éteindre les autres serveurs présents dans le même rack.
\item[Automatically Power On Server:] Si \emph{Yes}, permet de remettre automatiquement sous tension la lame après une coupure de courant.
\end{description}

\subsubsection{Média virtuel}

\subsubsection{Console distante}

\section{Installation de CentOS} \label{sec:installcentos}
\subsection{Média d'installation} \label{sec:installmediacentos}
\subsubsection{La lame a accès à Internet}
Dans le cas où la lame a accès à Internet, on réalisera une installation par le réseau, plus rapide et permettant d'obtenir les dernières versions des paquets.
Les images ISOs peuvent être obtenus de plusieurs miroirs\footnote{La liste à jour de tout les mirroirs de CentOS est disponible à l'adresse http://www.centos.org/modules/tinycontent/index.php?id=30} mais on préférera celui de l'IN2P3, connecté directement au réseau RENATER et qui offre donc d'excellents débits (de l'ordre de la dizaine de Mo/s).\newline
On téléchargera l'image ISO netinstall à l'adresse suivante:
\begin{verbatim}
http://mirror.in2p3.fr/linux/CentOS/[version]/isos/{i386,x86_64}/
\end{verbatim}

\noindent Le dossier contenant l'image d'installation est quand à lui situé à l'adresse:
\begin{verbatim}
http://mirror.in2p3.fr/linux/CentOS/[version]/os/{i386,x86_64}/
\end{verbatim}

\noindent Par exemple pour CentOS 6.3 x64 les URLs sont:
\begin{verbatim}
http://mirror.in2p3.fr/linux/CentOS/6.3/isos/x86_64/CentOS-6.3-x86_64-netinstall.iso
http://mirror.in2p3.fr/linux/CentOS/6.3/os/x86_64/
\end{verbatim}

\subsubsection{La lame n'a pas accès à Internet}
Dans le cas où la lame n'aurais pas accès à Internet ou que le lien soit en mode trunk (il est compliqué de configurer les VLANs pendant l'installation) il faut effectuer une installation avec le DVD contenant tout les paquets.\newline
L'image ISO sera préférablement téléchargé depuis le miroir de l'IN2P3 (voir paragraphe précèdent) à cette adresse:
\begin{verbatim}
http://mirror.in2p3.fr/linux/CentOS/[version]/isos/{i386,x86_64}/
\end{verbatim}

\noindent Par exemple pour CentOS 6.3 x64 l'url du DVD est:
\begin{verbatim}
http://mirror.in2p3.fr/linux/CentOS/6.3/isos/x86_64/CentOS-6.3-x86_64-bin-DVD1.iso
\end{verbatim}

\subsection{Options de boot}
Afin qu'Anaconda, l'installateur de CentOS, détecte correctement les cartes FC et le multipath et que l'installation se fasse en mode texte, il est nécessaire d'entrer la ligne suivante au prompt boot (ECHAP pour y accèder à partir de la version 6) :
\begin{verbatim} linux text mpath \end{verbatim}

\subsection{Configuration générale}
\subsubsection{Langue et clavier}
On choisira la langue que l'on souhaite mais l'Anglais est préférable car les messages d'erreurs y sont souvent plus explicites et on évite les traductions hasardeuses.\newline
Pour le clavier choisir le layout correspondant. Pour les claviers AZERTY on peut utiliser le layout fr ou fr-latin9 (avec la touche Euro).

\subsubsection{Méthode d'installation}
\textbf{Image netinstall}\newline
Si l'on utilise l'image ISO netinstall il faut choisir "URL" au dialogue "Installation Method".\newline
Dans le dialogue suivant qui demande de configurer la carté réseau on choisira celle qui est relié à Internet.
Pour IPv4 on préferera configurer l'adresse en statique.
Bien que début 2013 l'IPv6 ne soit pas encore routé sur le réseau de l'IUT son support peut-être laissé activé en prévision du futur.\newline
Si l'on souhaite agréger plusieurs cartes réseaux, on n'en configurera qu'une seule pour l'instant et l’agrégation sera réalisé une fois le système installé.\newline
Dans le dialogue demandant l'URL contenant l'image d'installation on rentrera celle indiquée dans la section \ref{sec:installmediacentos}.\newline
\begin{figure}[!h]
\centering
\includegraphics[resolution=110]{images/config_in2p3.png}
\caption{Exemple de configuration de la source pour CentOS 6.3 x64 en utilisant le mirroir de l'IN2P3}
\end{figure}

\textbf{Image DVD}\newline
Si l'on utilise l'image ISO DVD on choisira ???????,, au dialogue "Installation Method".\newline
Le dialogue suivant demande de configurer la carte réseau.
Si il y a des VLANs à configurer, on laissera la configuration automatique (DHCP) échouer et on choisira de configurer le réseau plus tard.
Sinon on configurera les adresses IPv4 et IPv6.\newline
Note: Si l'on souhaite agréger plusieurs cartes réseaux, on n'en configurera qu'une seule pour l'instant et agrégation sera réalisé une fois le système installé.\newline

\subsubsection{VNC}
A partir de la version 5 de CentOS l'installateur propose d'effectuer l'installation via VNC afin de proposer un partitionnement personnaliser ou de modifier les paquets à installer.
Étant donné que l'on souhaite installer un système minimal sur du stockage vide on continuera l'installation en mode texte.

\subsubsection{Fuseau horaire}
Linux sera le seul système installer "en dur" sur la lame, on peut donc laisser l'horloge système en UTC\footnote{EXPLIQUER BLABLABLABLABLA}.
Attention à bien choisir le bon fuseau horaire afin que le passage à l'heure d'été se fasse correctement et que le timestamp des logs soit cohérent entre les machines.

\subsubsection{Mot de passe}
Rien de spécial ici, il faut juste choisir un mot de passe sûr. Des sites comme random.org\footnote{random.org est un site internet intéressant pour l'obtention de nombres et de chaines aléatoires puisqu'il utilise le bruit atmosphérique pour les générer.} permettent de générer des mots de passes aléatoires.

\subsubsection{Partitionnement}
On souhaite installer le système sur du stockage vierge, on choisira donc d'utiliser le disque entier.
Si plusieurs disques sont présents on sélectionnera uniquement celui où le système résidera.
On configurera plus tard le stockage supplémentaire nécessaire aux images et aux instance virtuelles.

\subsubsection{Installation et premier redémarrage}
Après le partitionnement l'installation se déroule toute seule et l'installateur demande de confirmer le redémarrage à la fin.
Il faut bien penser à déconnecter le lecteur virtuel dans iLO afin d'éviter de redémarrer sur l'installation.\newline
A partir de ce moment, et si le réseau est configuré il est possible d'accéder à la machine en \gls{ssh}.

\subsection{Post-configuration}
Afin d'éviter de perdre la connexion il est préférable de modifier la configuration réseau depuis la console iLO plutôt que via SSH.

\subsubsection{Hostname}

\subsubsection{Agrégation de liens}

\subsubsection{VLANs}

\section{Installation de Solaris} \label{sec:installsolaris}

\chapter{Diagrammes Réseau et schémas de principe}
\section{Les différents services d'OpenStack}
\noindent 1. L'utilisateur demande la création d'une ou plusieurs instance.\\
2. OpenStack se charge de les crées et retourne les informations pour y accéder à l'utilisateur\\
3. L'utilisateur peut utiliser son instance\\
L'utilisateur final ne peut accéder qu'au dashboard.
\begin{center}
\includegraphics[scale=0.8,angle=90]{images/principeOpenStack-crop.pdf}
\end{center}
\section{Schémas global serveur dédié}
\section{Schémas global BladeCenter avec Solaris} \label{sch:glob}
\begin{center}
\includegraphics[scale=0.8,angle=90]{images/topoBladeSolaris-crop.pdf}
\end{center}
\section{Schémas global BladeCenter avec QEMU est GlusterFs}

\begin{center}
\includegraphics[scale=0.8,angle=90]{images/SchemaBladeQemuGlut-crop.pdf}
\end{center}

\chapter{Scripts d'automatisation}
\section{Solaris}
\subsection{nataddrule.sh}
%\begin{minted}[frame=single,fontfamily=courier,linenos,mathescape]{bash}
%\end{minted}

\section{OpenStack}
\subsection{keystone-?????.sh} \label{script:keystoneData}

\section{Exemple de configuration de MediaWiki avec Juju} \label{auto:juju}

Notre but est de déployer MediaWiki et d'y ajouter quelque services de haute disponibilité. Nous allons donc créer deux serveur web avec MediaWiki. Ils seront reliés à la même base MySql qui sera redondé avec une autre base esclave. La charge entre les deux serveurs web sera répartie par HAProxy. Le schéma ci-dessous présente la configuration final vu depuis juju-gui, l'interface web de Juju. Le logo vert en forme de globe représente le monde extérieur.
\begin{center}
\includegraphics[scale=0.75]{images/butJuju.png}
\end{center}

On commence par installer Juju et ses dépendances (Juju est disponible dans les dépôts officiels d'Ubuntu mais on préfère utiliser la version issu de sont ppa\footnote{Personal Package Archives. Ce sont des dépôts non-officiel mise à disposition des développeurs de logiciel libre par la plateforme LaunchPad} qui est plus à jour)
\begin{verbatim}
sudo apt-get -y install python-software-properties
sudo add-apt-repository ppa:juju/pkgs
sudo apt-get update && sudo apt-get -y install juju charm-tools
\end{verbatim}

On initialise ensuite Juju, on renseigne les identifiants de connections au Cloud dans {\raise.17ex\hbox{$\scriptstyle\sim$}}/.juju/environments.yaml puis on relance le bootstap.\footnote{Voir https://juju.ubuntu.com/get-started/openstack/ pour des explications sur environments.yaml}
\begin{alltt}
juju bootstrap
# Edition de environments.yaml
juju bootstap
\end{alltt}

On installe maintenant la base MySql maitre et MediaWiki. On ajoute ensuite une relation entre eux
\begin{verbatim}
juju deploy mysql
juju deploy mediawiki
juju add-relation mediawiki:db mysql
\end{verbatim}

On ajoute une nouvelle base MySql (qu'on appel mysql-slave) et on la lie avec la précédente
\begin{verbatim}
juju deploy mysql mysql-slave
juju add-relation mysql:master mysql-slave:slave
\end{verbatim}

On crée une deuxième instance de notre MediaWiki
\begin{verbatim}
juju add-unit mediawiki
\end{verbatim}


Et on ajoute un proxy pour répartir la charge entre les deux MediaWiki
\begin{verbatim}
juju deploy haproxy
juju add-relation haproxy mediawiki
\end{verbatim}

Toute notre infrastructure est crée mais n'est pas encore accessible depuis internet. 
Par défaut les machines ne peuvent pas communiquer avec l'extérieur. Il faut donc rendre le proxy accessible
Il suffit ensuite d'attendre que toutes les machines démarrent et que les ports s'ouvre
\begin{verbatim}
juju expose haproxy
juju status
\end{verbatim}

La dernière commande affiche un compte-rendu de toutes les machines créées et de tous les services installé. Il faut localiser la partie de HAProxy et regarder les ligne open-ports et public-address. Dans l'exemple ci-dessous, on voit que le port 80 est ouvert et qu'on peut accéder à la machine via l'ip 15.185.120.12. Si ces information n'ont pas encore apparu, c'est que la machine n'a pas encore fini d'être configuré. 
\begin{alltt}
[...]
services:
  haproxy:
    charm: cs:precise/haproxy-16
    exposed: true
    relations:
      reverseproxy:
      - mediawiki
    units:
      haproxy/0:
        agent-state: started
        machine: 5
        open-ports:
        - 80/tcp
        \hl{public-address: 15.185.120.126}
[...]
\end{alltt}

Voilà, notre réseau est terminé et fonctionnel. Si la charge augmente on peut ajouter des serveur web, des bases mysql esclave, voir des HAProxy avec la commande \emph{juju add-unit}\\
On peut aussi modifier la configuration un charmes. Par example, pour modifier le nom de notre MediaWiki, il suffit de faire \emph{juju set mediawiki name='Lamas Wiki!'}\\
La liste complète des charmes et de leurs configuration est disponible à l'adresse http://jujucharms.com/charms.
On peut aussi créé ces propres charmes. Il est possible de les développer dans de nombreux langages tel que le python, le ruby ou même avec puppet.
\newpage
\section{Example de configuration avec Puppet} \label{auto:puppet}
Notre but est de déployer une application sur un parc entier de machine.
Nous allons copier automatiquement un fichier sur deux machines.

On considère que PuppetMaster est installé sur le serveur et que les client ont puppet installé aussi.


On commence par identifier le client auprès du serveur :
Sur le client
\begin{verbatim}
sudo puppetd -t -v -w 60
\end{verbatim}

Et sur le serveur
\begin{verbatim}
sudo puppetca --list
  "dummy42" (D4:E7:37:A4:DF:F1:18:16:FA:D1:5B:38:CA:13:37:FB)
\end{verbatim}

Cette commande nous montre la liste des machines qui se sont identifier au serveur.
Ici la machine "dummy42" s'est identifier.
On l'enregistre
\begin{verbatim}
sudo puppetca --sign dummy42
\end{verbatim}

On ajoute une deuxième machine (dummy43 par example)
\begin{verbatim}
sudo puppetca --list
  "dummy43" (D4:E7:37:A4:DF:F1:18:16:FA:D1:5B:38:CA:13:37:FB)
sudo puppetca --sign dummy43
\end{verbatim}

Maintenant on a plus qu'à activer Puppet au boot de la machine
Dans /etc/default/puppet
\begin{verbatim}
# Start puppet on boot?
START=yes
\end{verbatim}
Et de le démarrer en tache de fond
\begin{verbatim}
sudo /etc/init.d/puppet start
\end{verbatim}

Puppet gère le parc de machines grâce au ficher /etc/puppet/manifests/site.pp. C'est dans celui-ci que l'on défini l'adresse du PupperMaster et le fichier qui contient la définition des machines. On le rempli donc ainsi, en supposant que l'adresse ip du PuppetMaster soit 192.168.1.1.
\begin{verbatim}
filebucket { 'main': server => '192.168.1.1' }
File { backup => 'main' }
 
import "node"
\end{verbatim}

Viens maintenant la définition des machines. On donc appliquer à dummy42 et dummy43 la configuration de vim. On écrit donc dans /etc/puppet/manifests/node.pp
\begin{verbatim}
node 'dummy42','dummy43 {
    include pushText
}
\end{verbatim}

On crée maintenant notre module. Puppet appel module, la liste des opération à effectuer pour compléter une tâche. Par example l'installation d'apache ou en l'occurrence, la copie d'un fichier.

On commence par crée l'arborescence de notre module.
\begin{verbatim}
mkdir -p /etc/puppet/modules/pushText/manifests
mkdir -p /etc/puppet/modules/pushText/files
\end{verbatim}

Puis on défini les action à faire. On crée un fichier puppet.txt dans /etc qui appartient a root et on lui applique des droits 644. Le fichier sera copié depuis /etc/puppet/module/pushText/puppet.txt 
\begin{verbatim}
class dummy {
        file { "/etc/puppet.txt":
                owner => root,
                group => root,
                mode => 644,
                source => "puppet:///pushText/puppet.txt"
        }
}
\end{verbatim}

On redémarre PuppetMaster pour qu'il prennent en compte les modification
\begin{verbatim}
sudo service puppetmaster restart
\end{verbatim}

Et on a plus qu'à forcer la mise a jour sur les clients.
\begin{verbatim}
sudo puppetd -t -v
\end{verbatim}

Ici, nous avons effectué une tâche très simple, mais puppet nous permet de faire plein de choses beaucoup plus poussé. On peut installer toute sorte de logiciel aussi bien Apache, Munin ou encore Wordpress. 
On peut même installer des modules d'Openstack.
Le clients Puppet est disponible sous plein de platforme comme Linux, Windows et Mac.
Toutes les recettes sont recensé dans la forge. Puppet\footnote{http://forge.puppetlabs.com/} et peuvent être installé facilement.
Par exemple, la figure ci-dessous montre une partie de la recette d'installation de vim, un célèbre éditeur de texte sous linux.

\begin{figure}[!h]
\begin{center}
\begin{Verbatim}[frame=single]
class vim inherits vim::params {
  exec { 'update-alternatives':
    command => 'update-alternatives --set editor /usr/bin/vim.basic',
    unless  => 'test /etc/alternatives/editor -ef /usr/bin/vim.basic',
  }

  file { '/etc/vim/vimrc':
    owner   => 'root',
    group   => 'root',
    mode    => '0644',
    source  => "puppet:///modules/vim/\${::lsbdistcodename}/etc/vim/vimrc",
    notify  => Exec['update-alternatives'],
    require => Package['vim'],
  }

  package { 'vim':
    ensure => present,
  }
}
\end{Verbatim}
\caption*{Exemple du manifeste de la recette de vim}
\end{center}
\end{figure}

\chapter{Fichiers de configuration OpenStack}
Ne sont inclus ici que le fichiers de configurations que nous avons modifié par rapport à l'installation par défaut d'OpenStack Folsom sur Centos 6.3.
\section{Keystone} \label{conf:Keystone}
\subsection{keystone.conf}
\begin{verbatim}
[DEFAULT]
# /etc/keystone/keystone.conf
# 02/03/2013

admin_token = e2e2f51aaf1ba336c736

bind_host    = 0.0.0.0
admin_port   = 35357
public_port  = 5000
compute_port = 8774

# === Logging Options ===
verbose = True
debug   = False

log_file = keystone.log
log_dir  = /var/log/keystone

[sql]
connection = mysql://keystone:ab9kcd@raijin/keystone

[identity]
driver = keystone.identity.backends.sql.Identity

[catalog]
template_file = /etc/keystone/default_catalog.templates
driver = keystone.catalog.backends.sql.Catalog

[token]
driver = keystone.token.backends.sql.Token

[ec2]
driver = keystone.contrib.ec2.backends.sql.Ec2

[ssl]
enable = False

# Par soucis de place l'ensemble des sections [filter:] a été omise,  il suffit de recopier
la partie correspondante du fichier de configuration d'origine
\end{verbatim}

\section{Glance} \label{conf:Glance}

\section{Swift} \label{conf:Swift}

\section{Nova} \label{conf:Nova}

\chapter{Fichiers de configuration Solaris}
\section{NAT}
\subsection{ipf.conf} \label{conf:NAT}
\section{Reverse Apache} \label{conf:apacheProxy}

\nocite{*}
\printbibliography

\deftranslation{Glossary}{Glossaire}
\printglossaries


% Le résumé
% Pas de glossaire dans le résumé
\begin{abstract}
Derrière le mot \gls{cloudcomputing} se trouvent de nombreux thèmes de l'informatique tels que la \gls{virtualisation}, la haute disponibilité, et la scalabilité.
La mise en place d'un \gls{cloudprive} est donc un sujet d'étude particulièrement intéressant en Réseaux \& Télécommunications.\newline
A l'aide d'\gls{openstack} nous avons mis en place une Infrastructure en tant que Service, ce qui nous as permis de mieux comprendre l'interaction entre les différents service (Réseau, Stockage, \Gls{virtualisation}).
L'installation a été effectué sur des systèmes UNIX (Solaris) et Linux (CentOS, Ubuntu Server) dont les choix sont expliqués dans ce rapport.\newline
En plus des services d'OpenStack nous avons intégrés une gestion de configuration automatique pour les instances virtuelles à l'aide de Puppet.
La mise en place de monitoring à l'aide de Shinken et Munin est également détaillé.\newline\newline

\noindent Behind what we call \gls{cloudcomputing} there is a bunch of interesting topics like virtualization, high-disponibility, and scalability.
This make deploying a Private Cloud an interesting studies subject in Networking \& Telecommunications.\newline
We used OpenStack, an open-source software, to create an Infrastructure as a Service, this gave us a better understanding of service's interaction (Networking, Storage, Virtualization).
Setup was made on both UNIX (Solaris) and Linux (CentOS, Ubuntu Server) systems whose choice are explained in this report.\newline
In additions of OpenStack services we used Puppet to deliver automated configuration deployment to VMs.
Monitoring with Shinken and Munin is also detailled.

\end{abstract}

\end{document}
